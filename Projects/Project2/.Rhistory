library(mxnet)
library(mlbench)
setwd('C:\\Users\\avina\\Desktop\\prudential data')
review <- read.csv('Amazon_Reviews.csv')
review <- review[,3:7]
review
review[,5] = as.numeric(review[,5])-1
trainData <- review[1:2000,]
testData <- review[2000:5000,]
library(mxnet)
library(mlbench)
setwd('C:\\Users\\avina\\Desktop\\prudential data\\Final Project')
review <- read.csv('Amazon_Reviews.csv')
review <- review[,3:7]
review
review[,5] = as.numeric(review[,5])-1
trainData <- review[1:2000,]
testData <- review[2000:5000,]
train.x <- data.matrix(trainData[,1:5])
train.x
train.y <- trainData[,5]
test.x <- data.matrix(testData[,])
test.y <- testData[,5]
mx.set.seed(0)
model <- mx.mlp(train.x, train.y , hidden_node=10, out_node=2,
learning.rate=0.07)
plot(model)
plot.nnnet(model)
plot.nnet(model)
plot(model)
library(mxnet)
library(mlbench)
library(deepnet)
review <- read.csv('data_sparse.csv')
setwd('C:\\Users\\avina\\Desktop\\prudential data')
review <- review[,3:7]
names(review)
setwd('C:\\Users\\avina\\Desktop\\prudential data')
review <- read.csv('data_sparse.csv')
review <- review[,3:7]
review <- read.csv('data_sparse.csv')
review <- review[,1:579]
names(review)
review[,580] = as.numeric(review[,580])-1
review[,580] = as.numeric(review[,580])-1
review <- read.csv('data_sparse.csv')
names(review)
review[,580] = as.numeric(review[,580])-1
trainData <- review[1:3000,]
testData <- review[3001:5000,]
trainData <- data.matrix(trainData)
testData <- data.matrix(testData)
train.x <- trainData[,1:579]
train.x
train.y <- trainData[,580]
train.x <- t(train.x)
test <- t(testData)
table(train.y)
result <- mx.symbol.Variable('result')
poswrds <- mx.symbol.Variable('PositiveWordsCount')
negwrds <- mx.symbol.Variable('NegativeWordsCount')
sentpos <- mx.symbol.Variable('Sentiment_Positive')
sentbeg <- mx.symbol.Variable('Sentiment_Negative')
conv1 <- mx.symbol.Convolution(data=poswrds, kernel=c(5,5), num_filter=20, pad=c(2,2))
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max",
kernel=c(2,2), stride=c(2,2))
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max",
kernel=c(2,2), stride=c(2,2))
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
train.array <- train.x
dim(train.array) <- c(2, 2, 1, ncol(train.x))
ncol(train.x)
dim(train.array) <- c(3, 193, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(1, 5, 1, ncol(test))
ncol(test)
dim(test.array) <- c(10, 58, 1, ncol(test))
n.gpu <- 1
device.cpu <- mx.cpu()
device.gpu <- lapply(0:(n.gpu-1), function(i) {
mx.gpu(i)
})
train.x
mx.set.seed(0)
tic <- proc.time()
model1 <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
ctx=device.cpu,
num.round=1,
array.batch.size=10,
learning.rate=0.05, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
epoch.end.callback=mx.callback.log.train.metric(100))
library(mxnet)
library(mlbench)
library(deepnet)
setwd('C:\\Users\\avina\\Desktop\\prudential data\\Final Project')
review <- read.csv('SparseRepresentation.csv')
names(review)
review[,580] = as.numeric(review[,580])
review[c(581:625)] <- 0
trainData <- review[1:3000,]
testData <- review[3001:5000,]
trainData <- data.matrix(trainData)
testData <- data.matrix(testData)
View(trainData)
train.x <- trainData[,]
train.x
train.y <- trainData[,580]
train.x <- t(train.x)
test <- t(testData)
table(train.y)
data <- mx.symbol.Variable('data')
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20, pad=c(2,2))
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max",
kernel=c(2,2), stride=c(2,2))
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max",
kernel=c(2,2), stride=c(2,2))
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
train.array <- train.x
dim(train.array) <- c(25, 25, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(25, 25, 1, ncol(test))
n.gpu <- 1
device.cpu <- mx.cpu()
device.gpu <- lapply(0:(n.gpu-1), function(i) {
mx.gpu(i)
})
train.x
mx.set.seed(100)
tic <- proc.time()
model1 <- mx.model.FeedForward.create(lenet, X=train.array, y=as.array(train.y),
ctx=device.cpu,
num.round=10,
array.batch.size=50,
learning.rate=0.05, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
batch.end.callback=mx.callback.log.train.metric(100))
preds <- predict(model1, test.array)
pred.label <- max.col(t(preds)) - 1
library(caret)
confusionMatrix(pred.label, testData[,580])
write.csv(preds,'CNN.csv')
plot(model1)
plot(X,y)
plot(train.array,train.y)
model1 <- mx.model.FeedForward.create(lenet, X=train.array, y=as.array(train.y),
ctx=device.cpu,
num.round=3,
array.batch.size=50,
learning.rate=0.05, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
batch.end.callback=mx.callback.log.train.metric(100))
preds <- predict(model1, test.array)
pred.label <- max.col(t(preds)) - 1
library(caret)
confusionMatrix(pred.label, testData[,580])
library(caret)
library(e1071)
setwd('C:\\Users\\avina\\Desktop\\prudential data\\Final Project')
review <- read.csv('Amazon_Reviews.csv')
review[,7] = as.numeric(review[,7])-1
review[,7]
review[,7]
result<-review[,7]
data <- read.csv('data_sparse.csv')
data <- cbind(data,result)
trainData <- data[1:3000,]
testData <- data[3001:5000,]
data <- read.csv('SparseRepresentation.csv')
data <- cbind(data,result)
testData <- data[3001:5000,]
trainData <- data[1:3000,]
model = svm(result ~ ., kernel = "radial", cost = 4,
type="C-classification", gamma =0.125, data = trainData, scale = F)
model
predictions <-  predict(model, testData)
plot(predictions)
confusionMatrix(testData[,580], predictions)
table(testData[,580], predictions)
write.csv(predictions,'SVM.csv')
getwd()
library(tm)
library(RTextTools)
library(plyr)
library('stringr')
library('dplyr')
library(tidytext)
install.packages('dplyr')
install.packages('tidytext')
library(tm)
library(RTextTools)
library(plyr)
library('stringr')
library('dplyr')
library(tidytext)
review <- read.csv("C:\\Users\\avina\\Desktop\\prudential data\\Final Project\\Amazon_Comments.csv")
Tokenize_Review <- function(string){
# Change all the words to lowercase
t_re <- tolower(string)
# Remove everything that is not a number or letter
t_re <- stringr::str_replace_all(t_re,"[^a-zA-Z\\s]", " ")
# Shrink down to just one white space
t_re <- stringr::str_replace_all(t_re,"[\\s]+", " ")
# Split the data
t_re <- stringr::str_split(t_re, " ")[[1]]
# Get rid of trailing "" if necessary
i <- which(t_re == "")
if(length(i) > 0){
t_re <- t_re[-i]
}
print(t_re)
return(t_re)
}
tokenized_data = Corpus(DataframeSource(review))
cleanData <- tm_map(tokenized_data,Tokenize_Review)
cleanData
plaindata = tm_map(cleanData, PlainTextDocument)
plaindata
tokenized_data = tm_map(plaindata, removeWords, c("the","and","you","for","that","this","are","was","your","has","there","had","they","our","them","were","hey","of","is","to", stopwords("english")))
StemDocs <- tm_map(tokenized_data, stemDocument)
StemDocs
dtm = DocumentTermMatrix(StemDocs)
dtm
ncol(dtm)
nrow(dtm)
findFreqTerms(dtm, lowfreq=1)
sparse = removeSparseTerms(dtm, 0.99)
dataSparse = as.data.frame(as.matrix(sparse), row.names = F)
View(dataSparse)
library(NLP)
library(Rstem)
require(quanteda)
positive = read.table("positive-words.txt", sep="\t")
negative = read.table("negative-words.txt", sep="\t")
stem <- function(list){
for(i in c(1:nrow(list))){
list[i,1] <- wordStem(String(list[i,1]))
}
list <- unique(list[,1] )
}
positive <- stem(positive)
negative <- stem(negative)
for(level in c(1:nrow(review))){
str <- String(review$review[level])
tokens <- str[wordpunct_tokenizer(str)]
stem <- wordStem(tokens)
positiveCount <- length(intersect(stem,positive))
negativeCount <- length(intersect(stem,negative))
review$PositiveWordsCount[level] <- positiveCount
review$NegativeWordsCount[level] <- negativeCount
if((positiveCount + negativeCount) == 0){
positiveCount <- 1
negativeCount <- 1
}
review$Sentiment_Positive[level] <- positiveCount / (positiveCount + negativeCount)
review$Sentiment_Negative[level] <- negativeCount / (positiveCount + negativeCount)
if(positiveCount >= negativeCount){
review$result[level] <- "Positive"
} else {
review$result[level] <- "Negative"
}
}
review <- read.csv("Amazon_Reviews.csv")
result1 <- as.numeric(review$result)-1
dataSparse <- cbind(dataSparse,result1)
library(mxnet)
library(mlbench)
library(darch)
review <- read.csv('SparseRepresentation.csv')
review[,580] = as.numeric(review[,580])-1
review
trainData <- review[1:2000,]
testData <- review[2001:5000,]
train.x <- data.matrix(trainData[,1:579])
train.x
train.y <- trainData[,580]
train.y
test.x <- data.frame(testData[,1:579])
test.x
test.y <- testData[,580]
test.y
library(deepnet)
dnn <- dbn.dnn.train(train.x, train.y, hidden = c(3,3),activationfun = "sigm", output = "sigm", numepochs = 100, cd=2)
summary(dnn)
err.dnn <- nn.test(dnn, test.x, test.y)
err.dnn
yy.dnn <- nn.predict(dnn, test.x)
summary(yy.dnn)
head(yy.dnn)
library(Lahman)
library(SDMTools)
install.packages("Lahman")
library(Lahman)
library(SDMTools)
accuracy(test.y,yy.dnn,0.5)
confusion.matrix(test.y,yy.dnn,threshold=0.5)
confusion.matrix(test.y, yy.dnn)
write.csv(yy.dnn,'DBN_Out.csv')
m <- round(yy.dnn)
table( yy.dnn, test.y)
plot(dnn)
getwd()
library(mxnet)
library(mlbench)
library(deepnet)
setwd('C:\\Users\\avina\\Desktop\\prudential data\\Final Project')
review <- read.csv('SparseRepresentation.csv')
names(review)
review[,580] = as.numeric(review[,580])
review[c(581:625)] <- 0
trainData <- review[1:3000,]
testData <- review[3001:5000,]
trainData <- data.matrix(trainData)
testData <- data.matrix(testData)
View(trainData)
train.x <- trainData[,]
train.x
train.y <- trainData[,580]
train.x <- t(train.x)
test <- t(testData)
table(train.y)
data <- mx.symbol.Variable('data')
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20, pad=c(2,2))
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max",
kernel=c(2,2), stride=c(2,2))
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max",
kernel=c(2,2), stride=c(2,2))
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
train.array <- train.x
dim(train.array) <- c(25, 25, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(25, 25, 1, ncol(test))
n.gpu <- 1
device.cpu <- mx.cpu()
device.gpu <- lapply(0:(n.gpu-1), function(i) {
mx.gpu(i)
})
train.x
mx.set.seed(100)
tic <- proc.time()
model1 <- mx.model.FeedForward.create(lenet, X=train.array, y=as.array(train.y),
ctx=device.cpu,
num.round=3,
array.batch.size=50,
learning.rate=0.05, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
batch.end.callback=mx.callback.log.train.metric(100))
model1 <- mx.model.FeedForward.create(lenet, X=train.array, y=as.array(train.y),
ctx=device.cpu,
num.round=2,
array.batch.size=50,
learning.rate=0.05, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
batch.end.callback=mx.callback.log.train.metric(100))
